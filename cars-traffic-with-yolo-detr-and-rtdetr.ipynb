{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10138583,"sourceType":"datasetVersion","datasetId":6257224}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install ultralytics","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:26:20.223221Z","iopub.execute_input":"2024-12-08T13:26:20.223900Z","iopub.status.idle":"2024-12-08T13:26:30.362364Z","shell.execute_reply.started":"2024-12-08T13:26:20.223868Z","shell.execute_reply":"2024-12-08T13:26:30.361504Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\nfrom collections import Counter","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:26:30.363987Z","iopub.execute_input":"2024-12-08T13:26:30.364237Z","iopub.status.idle":"2024-12-08T13:26:33.891443Z","shell.execute_reply.started":"2024-12-08T13:26:30.364212Z","shell.execute_reply":"2024-12-08T13:26:33.890568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = YOLO('yolov8n.pt')  # Use 'yolov8n.pt', 'yolov8s.pt', etc., depending on the model size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:27:33.702321Z","iopub.execute_input":"2024-12-08T13:27:33.703088Z","iopub.status.idle":"2024-12-08T13:27:33.752587Z","shell.execute_reply.started":"2024-12-08T13:27:33.703052Z","shell.execute_reply":"2024-12-08T13:27:33.751947Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Video input and output paths\nvideo_path = \"/kaggle/input/cars-traffic/3.mp4\"\noutput_path = \"/kaggle/working/output_video.mp4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:27:38.066307Z","iopub.execute_input":"2024-12-08T13:27:38.066686Z","iopub.status.idle":"2024-12-08T13:27:38.070843Z","shell.execute_reply.started":"2024-12-08T13:27:38.066625Z","shell.execute_reply":"2024-12-08T13:27:38.069896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Open video file\ncap = cv2.VideoCapture(video_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\n# Video writer for saving the output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:27:47.215128Z","iopub.execute_input":"2024-12-08T13:27:47.215695Z","iopub.status.idle":"2024-12-08T13:27:47.306178Z","shell.execute_reply.started":"2024-12-08T13:27:47.215664Z","shell.execute_reply":"2024-12-08T13:27:47.305207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"car_count = 0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Run YOLOv8 on the frame\n    results = model(frame)\n\n    # Extract detections for \"car\" class\n    for box in results[0].boxes:  # Access the first result (since batch size = 1)\n        class_id = int(box.cls)   # Class ID\n        confidence = float(box.conf)  # Confidence score\n        if confidence > 0.5:  # Confidence threshold\n            if class_id == 2:  # Assuming '2' is the ID for 'car'\n                car_count += 1\n                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Coordinates\n                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n                cv2.putText(frame, \"Car\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Display total car count on the video\n    cv2.putText(frame, f\"Car Count: {car_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n\n    # Write the frame to output video\n    out.write(frame)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:29:13.683708Z","iopub.execute_input":"2024-12-08T13:29:13.684466Z","iopub.status.idle":"2024-12-08T13:29:23.437462Z","shell.execute_reply.started":"2024-12-08T13:29:13.684432Z","shell.execute_reply":"2024-12-08T13:29:23.436674Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Release video objects\ncap.release()\nout.release()\n\nprint(f\"Processing complete. Output saved to {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:29:45.111238Z","iopub.execute_input":"2024-12-08T13:29:45.112091Z","iopub.status.idle":"2024-12-08T13:29:45.118178Z","shell.execute_reply.started":"2024-12-08T13:29:45.112057Z","shell.execute_reply":"2024-12-08T13:29:45.117454Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Single Frame Processing","metadata":{}},{"cell_type":"code","source":"import torch\nimport cv2\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\n\n# Load the pre-trained YOLOv5 model (e.g., YOLOv5s)\nmodel = YOLO('yolov5s.pt')  # You can choose the model variant: yolov5s, yolov5m, yolov5l, yolov5x\n\n# Open the video file (for single frame extraction)\nvideo_path = \"/kaggle/input/cars-traffic/3.mp4\"  # Replace with your video path\ncap = cv2.VideoCapture(video_path)\n\nif not cap.isOpened():\n    print(\"Error: Couldn't open video.\")\n    exit()\n\n# Capture a single frame (first frame)\nret, frame = cap.read()\nif not ret:\n    print(\"Failed to capture the first frame.\")\n    exit()\n\n# Process the captured frame with YOLO\nresults = model(frame)  # Run inference on the frame\n\n# Extract bounding boxes and labels from the YOLO output\nlabels = results.names  # Class labels (e.g., 'car', 'truck')\nboxes = results.xywh[0].cpu().numpy()  # Get bounding boxes in xywh format (normalized)\n\n# Filter out detections for 'car' (label 2 corresponds to 'car' in YOLOv5 COCO dataset)\ncar_count = 0\nfor box in boxes:\n    x_center, y_center, width, height, conf, class_id = box\n    if labels[int(class_id)] == 'car':  # 'car' label in YOLO\n        car_count += 1\n        x1 = int((x_center - width / 2) * frame.shape[1])\n        y1 = int((y_center - height / 2) * frame.shape[0])\n        x2 = int((x_center + width / 2) * frame.shape[1])\n        y2 = int((y_center + height / 2) * frame.shape[0])\n\n        # Draw the bounding box around the car\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        cv2.putText(frame, \"Car\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Display the frame with detections\ncv2.putText(frame, f\"Car Count: {car_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n\n# Convert to RGB for matplotlib (since OpenCV uses BGR)\nframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n# Show the frame with bounding boxes and car count\nplt.imshow(frame_rgb)\nplt.title(f\"Car Count: {car_count}\")\nplt.axis(\"off\")\nplt.show()\n\n# Optionally, save the frame with detections\noutput_path = \"/path/to/output_frame.jpg\"  # Replace with your desired output path\ncv2.imwrite(output_path, frame)\n\n# Clean up\ncap.release()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Detr Model","metadata":{}},{"cell_type":"code","source":"!pip install torch torchvision","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:32:26.153990Z","iopub.execute_input":"2024-12-08T13:32:26.154863Z","iopub.status.idle":"2024-12-08T13:32:34.406840Z","shell.execute_reply.started":"2024-12-08T13:32:26.154828Z","shell.execute_reply":"2024-12-08T13:32:34.405703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:37:47.466965Z","iopub.execute_input":"2024-12-08T13:37:47.467761Z","iopub.status.idle":"2024-12-08T13:37:55.702961Z","shell.execute_reply.started":"2024-12-08T13:37:47.467727Z","shell.execute_reply":"2024-12-08T13:37:55.701808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nfrom transformers import DetrImageProcessor, DetrForObjectDetection","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:38:00.554368Z","iopub.execute_input":"2024-12-08T13:38:00.555325Z","iopub.status.idle":"2024-12-08T13:38:14.464393Z","shell.execute_reply.started":"2024-12-08T13:38:00.555270Z","shell.execute_reply":"2024-12-08T13:38:14.463702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load pretrained DETR model and processor\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:38:14.466064Z","iopub.execute_input":"2024-12-08T13:38:14.466713Z","iopub.status.idle":"2024-12-08T13:38:19.541115Z","shell.execute_reply.started":"2024-12-08T13:38:14.466668Z","shell.execute_reply":"2024-12-08T13:38:19.539897Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_path = \"/kaggle/input/cars-traffic/3.mp4\"\noutput_path = \"/kaggle/working/output_video_detr.mp4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:38:32.665144Z","iopub.execute_input":"2024-12-08T13:38:32.665503Z","iopub.status.idle":"2024-12-08T13:38:32.669571Z","shell.execute_reply.started":"2024-12-08T13:38:32.665473Z","shell.execute_reply":"2024-12-08T13:38:32.668714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Single Frame Processing","metadata":{}},{"cell_type":"code","source":"import torch\nimport cv2\nimport matplotlib.pyplot as plt\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\n# Load the pre-trained DETR model and processor\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n# Open video file and capture a frame\nvideo_path = \"/kaggle/input/cars-traffic/3.mp4\"\ncap = cv2.VideoCapture(video_path)\nret, frame = cap.read()\ncap.release()\n\nif not ret:\n    print(\"Failed to capture a frame from the video.\")\nelse:\n    # Convert the captured frame from BGR (OpenCV format) to RGB (for DETR)\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # Preprocess the image\n    inputs = processor(images=frame_rgb, return_tensors=\"pt\")\n\n    # Perform object detection\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Post-process the results\n    target_sizes = torch.tensor([frame_rgb.shape[:2]])\n    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n\n    # Draw bounding boxes around detected cars\n    for box, label, score in zip(results[\"boxes\"], results[\"labels\"], results[\"scores\"]):\n        if label == 3:  # Label 3 corresponds to 'car' in COCO dataset\n            x1, y1, x2, y2 = map(int, box.tolist())\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, \"Car\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Convert frame back to RGB for displaying with matplotlib\n    frame_rgb_with_boxes = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # Display the frame with detections using matplotlib\n    plt.imshow(frame_rgb_with_boxes)\n    plt.axis('off')  # Hide axis\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T13:59:03.809740Z","iopub.execute_input":"2024-12-08T13:59:03.810461Z","iopub.status.idle":"2024-12-08T13:59:08.695780Z","shell.execute_reply.started":"2024-12-08T13:59:03.810431Z","shell.execute_reply":"2024-12-08T13:59:08.694846Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# full video Processing","metadata":{}},{"cell_type":"code","source":"import torch\nimport cv2\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\n# Load the pre-trained DETR model and processor\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n# Open video file\nvideo_path = \"/kaggle/input/cars-traffic/3.mp4\"\ncap = cv2.VideoCapture(video_path)\n\nif not cap.isOpened():\n    print(\"Error: Couldn't open video.\")\n    exit()\n\n# Initialize car count\ntotal_car_count = 0\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert frame from BGR (OpenCV) to RGB (for DETR)\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n    # Preprocess the frame\n    inputs = processor(images=rgb_frame, return_tensors=\"pt\")\n\n    # Perform object detection\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Post-process the results\n    target_sizes = torch.tensor([rgb_frame.shape[:2]])\n    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.7)[0]\n\n    # Count the vehicles (label 3 is for 'car' in COCO)\n    frame_car_count = 0\n    for label in results[\"labels\"]:\n        if label == 3:  # 'Car' label in COCO\n            frame_car_count += 1\n\n    # Add the count of cars in this frame to the total\n    total_car_count += frame_car_count\n\n    # Optionally, draw bounding boxes around detected cars in the frame\n    for box, label, score in zip(results[\"boxes\"], results[\"labels\"], results[\"scores\"]):\n        if label == 3:  # Label 3 corresponds to 'car'\n            x1, y1, x2, y2 = map(int, box.tolist())\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            cv2.putText(frame, \"Car\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Display the vehicle count on the frame\n    cv2.putText(frame, f\"Car Count: {frame_car_count}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n\n    # Optionally, show the frame\n    # cv2.imshow(\"Frame with Cars\", frame)\n    # if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n    #     break\n\n# After processing all frames\ncap.release()\n\nprint(f\"Total number of cars detected in the video: {total_car_count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T14:00:53.569021Z","iopub.execute_input":"2024-12-08T14:00:53.569387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# rtDetr","metadata":{}},{"cell_type":"code","source":"!pip install transformers torch torchvision opencv-python-headless","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:42:19.665530Z","iopub.execute_input":"2024-12-10T12:42:19.666187Z","iopub.status.idle":"2024-12-10T12:42:28.162697Z","shell.execute_reply.started":"2024-12-10T12:42:19.666135Z","shell.execute_reply":"2024-12-10T12:42:28.161634Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n\n# Load the RT-DETR model and image processor\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\nmodel = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd\")\nmodel.eval()\n\n# Path to input video\nvideo_path = '/kaggle/input/cars-traffic/3.mp4'\n\n# Open the video and read a single frame\ncap = cv2.VideoCapture(video_path)\nsuccess, frame = cap.read()\ncap.release()\n\nif not success:\n    raise ValueError(\"Could not read frame from video.\")\n\n# Convert the frame to a PIL Image\nimage = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n# Process the image with RT-DETR\ninputs = image_processor(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# Post-process detections\nresults = image_processor.post_process_object_detection(\n    outputs,\n    target_sizes=torch.tensor([(image.height, image.width)]),\n    threshold=0.3,\n)\n\n# Annotate the frame with bounding boxes and labels\nfor result in results:\n    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n        label = model.config.id2label[label_id.item()]\n        score = score.item()\n        box = [round(coord) for coord in box.tolist()]\n        x1, y1, x2, y2 = box\n        # Draw bounding box and label\n        frame = cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        frame = cv2.putText(\n            frame,\n            f\"{label} {score:.2f}\",\n            (x1, y1 - 10),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (0, 255, 0),\n            2,\n        )\n\n# Convert frame to RGB for display\nannotated_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n# Display the annotated frame\nplt.figure(figsize=(12, 8))\nplt.imshow(annotated_image)\nplt.axis(\"off\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:45:23.106931Z","iopub.execute_input":"2024-12-10T12:45:23.107355Z","iopub.status.idle":"2024-12-10T12:45:25.389004Z","shell.execute_reply.started":"2024-12-10T12:45:23.107316Z","shell.execute_reply":"2024-12-10T12:45:25.388029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport cv2\nfrom PIL import Image\nfrom transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n\n# Load the RT-DETR model and image processor\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\nmodel = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd\")\nmodel.eval()\n\n# Video paths\ninput_video_path = '/kaggle/input/cars-traffic/3.mp4'\noutput_video_path = '/kaggle/working/processed_output.mp4'\n\n# Open the input video\ncap = cv2.VideoCapture(input_video_path)\n\n# Get video properties\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define the video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert frame to PIL image\n    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n    # Process the image with RT-DETR\n    inputs = image_processor(images=image, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Post-process detections\n    results = image_processor.post_process_object_detection(\n        outputs,\n        target_sizes=torch.tensor([(image.height, image.width)]),\n        threshold=0.3,\n    )\n\n    # Draw bounding boxes and labels\n    for result in results:\n        for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n            label = model.config.id2label[label_id.item()]\n            score = score.item()\n            box = [round(coord) for coord in box.tolist()]\n            x1, y1, x2, y2 = box\n            # Draw the bounding box\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            # Draw the label and score\n            cv2.putText(\n                frame,\n                f\"{label} {score:.2f}\",\n                (x1, y1 - 10),\n                cv2.FONT_HERSHEY_SIMPLEX,\n                0.5,\n                (0, 255, 0),\n                2,\n            )\n\n    # Write the processed frame to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\nprint(f\"Processed video saved to {output_video_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:45:40.288043Z","iopub.execute_input":"2024-12-10T12:45:40.288898Z","iopub.status.idle":"2024-12-10T12:56:27.199975Z","shell.execute_reply.started":"2024-12-10T12:45:40.288861Z","shell.execute_reply":"2024-12-10T12:56:27.198994Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Video\nVideo('/kaggle/working/processed_output.mp4', embed=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-10T12:56:27.201851Z","iopub.execute_input":"2024-12-10T12:56:27.202247Z","iopub.status.idle":"2024-12-10T12:56:27.397623Z","shell.execute_reply.started":"2024-12-10T12:56:27.202207Z","shell.execute_reply":"2024-12-10T12:56:27.396029Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}